{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BRIEF : THE WHEEL OF EMOTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ressources\n",
    "https://www.actuia.com/contribution/victorbigand/tutoriel-tal-pour-les-debutants-classification-de-texte/ <br>\n",
    "https://realpython.com/sentiment-analysis-python/#how-classification-works <br>\n",
    "https://www.datacamp.com/community/tutorials/stemming-lemmatization-python <br>\n",
    "https://www.machinelearningplus.com/nlp/lemmatization-examples-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/helloworld/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/helloworld/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/helloworld/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB, CategoricalNB, ComplementNB, BernoulliNB\n",
    "from sklearn.decomposition import FastICA, KernelPCA, TruncatedSVD, SparsePCA, NMF, FactorAnalysis, LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import ngrams\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Emotion_final.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mise à l'échelle pour intégration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(data['Text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ntlk : Tokenisation, stopwords, lemmatization, n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopW = stopwords.words('english')\n",
    "\n",
    "exclude = set(string.punctuation)\n",
    "\n",
    "stopW.extend(exclude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "toktok = word_tokenize(str(x))\n",
    "\n",
    "tokens_without_stopW = [word for word in toktok if word not in stopW]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [lemma.lemmatize(lemma.lemmatize(lemma.lemmatize(word,pos='a'),pos='v'), pos='n')\n",
    "          for word in tokens_without_stopW]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(sent):\n",
    "    tokens = word_tokenize(sent.lower())\n",
    "    tokens = [lemma.lemmatize(lemma.lemmatize(lemma.lemmatize(word, pos='a'), pos='v'),\n",
    "                              pos='n') for word in tokens_without_stopW]\n",
    "    return ''.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = []\n",
    "bigrams = ngrams(tokens_without_stopW, 2)\n",
    "for words in bigrams:\n",
    "    x1.append(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words=stopW, analyzer='word', ngram_range=(1,2))\n",
    "\n",
    "X = vectorizer.fit_transform(x)\n",
    "\n",
    "features_names = vectorizer.get_feature_names()\n",
    "pd.DataFrame(X.toarray(), columns = features_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Emotion_final.csv\", names=['content','sentiment'], skiprows=1)\n",
    "df1 = df1 = pd.read_csv('text_emotion.csv', usecols=['sentiment','content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = np.array(df.content)\n",
    "targets = np.array(df.sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vobabulary analysis\n",
    "vec = CountVectorizer(stop_words=stopwords)\n",
    "X = vec.fit_transform(corpus)\n",
    "words = vec.get_feature_names()\n",
    "\n",
    "print(\"vocabulary size; %d\" % len(words) )\n",
    "\n",
    "# Compute rank\n",
    "wsum = np.array(X.sum(0))[0]\n",
    "ix = wsum.argsort()[::-1]\n",
    "wrank = wsum[ix] \n",
    "labels = [words[i] for i in ix]\n",
    "\n",
    "# Sub-sample the data to plot.\n",
    "# take the 20 first + the rest sample with the given step \n",
    "def subsample(x, step=150):\n",
    "    return np.hstack((x[:30], x[10::step]))\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20,8))\n",
    "freq = subsample(wrank)\n",
    "r = np.arange(len(freq))\n",
    "plt.bar(r, freq, width=0.7)\n",
    "plt.xticks(r, subsample(labels), rotation=55)\n",
    "plt.xlabel('word rank')\n",
    "plt.ylabel('word frequncy')\n",
    "plt.title(\"Words ordered by rank. The first rank is the most frequent words and the last one is the less present\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe1 = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('sgd', SGDClassifier()),\n",
    "])\n",
    "\n",
    "pipe2 = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('sgd', SGDClassifier()),\n",
    "])\n",
    "pipe3 = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('svml', LinearSVC()),\n",
    "])\n",
    "\n",
    "pipe4 = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('logit', LogisticRegression()),\n",
    "])\n",
    "\n",
    "pipe5 = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('compl_nb', ComplementNB()),\n",
    "])\n",
    "\n",
    "\n",
    "def run_pipes(pipes, splits=10, test_size=0.2, seed=42):  \n",
    "    res = defaultdict(list)\n",
    "    spliter = ShuffleSplit(n_splits=splits, test_size=test_size, random_state=seed)\n",
    "    for idx_train, idx_test in spliter.split(corpus):\n",
    "        for pipe in pipes:\n",
    "            # name of the model\n",
    "            name = \"-\".join([x[0] for x in pipe.steps])\n",
    "            \n",
    "            # extract datasets\n",
    "            X_train = corpus[idx_train]\n",
    "            X_test = corpus[idx_test]\n",
    "            y_train = targets[idx_train]\n",
    "            y_test = targets[idx_test]\n",
    "            \n",
    "            # Learn\n",
    "            start = time.time()\n",
    "            pipe.fit(X_train, y_train)\n",
    "            fit_time = time.time() - start\n",
    "            \n",
    "            # predict and save results\n",
    "            y = pipe.predict(X_test)\n",
    "            res[name].append([\n",
    "                fit_time,\n",
    "                f1_score(y_test,y, average='macro'),\n",
    "                precision_score(y_test,y, average='macro'),\n",
    "                recall_score(y_test,y, average='macro'),\n",
    "                \n",
    "\n",
    "\n",
    "            ])\n",
    "    return res\n",
    "\n",
    "def print_table(res):\n",
    "       # Compute mean and std\n",
    "       final = {}     \n",
    "       for model in res:         \n",
    "         arr = np.array(res[model])         \n",
    "         final[model] = {             \n",
    "             \"time\" : arr[:, 0].mean().round(2),             \n",
    "             \"f1_score\": [arr[:,2].mean().round(3), arr[:,2].std().round(3)],             \n",
    "             \"Precision\" : arr[:,2].mean().round(3),             \n",
    "             \"Recall\" : arr[:,2].mean().round(3)         }\n",
    "       df = pd.DataFrame.from_dict(final, orient=\"index\").round(3)\n",
    "       return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = run_pipes([pipe1, pipe2, pipe3, pipe4, pipe5], splits=1)\n",
    "print_table(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenation of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = pd.concat([df, df1], ignore_index=True)\n",
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = np.array(final.content)\n",
    "targets = np.array(final.sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe1 = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('sgd', SGDClassifier()),\n",
    "])\n",
    "\n",
    "pipe2 = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('sgd', SGDClassifier()),\n",
    "])\n",
    "pipe3 = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('svml', LinearSVC()),\n",
    "])\n",
    "\n",
    "pipe4 = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('logit', LogisticRegression()),\n",
    "])\n",
    "\n",
    "pipe5 = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('compl_nb', ComplementNB()),\n",
    "])\n",
    "\n",
    "\n",
    "def run_pipes(pipes, splits=10, test_size=0.2, seed=42):  \n",
    "    res = defaultdict(list)\n",
    "    spliter = ShuffleSplit(n_splits=splits, test_size=test_size, random_state=seed)\n",
    "    for idx_train, idx_test in spliter.split(corpus):\n",
    "        for pipe in pipes:\n",
    "            # name of the model\n",
    "            name = \"-\".join([x[0] for x in pipe.steps])\n",
    "            \n",
    "            # extract datasets\n",
    "            X_train = corpus[idx_train]\n",
    "            X_test = corpus[idx_test]\n",
    "            y_train = targets[idx_train]\n",
    "            y_test = targets[idx_test]\n",
    "            \n",
    "            # Learn\n",
    "            start = time.time()\n",
    "            pipe.fit(X_train, y_train)\n",
    "            fit_time = time.time() - start\n",
    "            \n",
    "            # predict and save results\n",
    "            y = pipe.predict(X_test)\n",
    "            res[name].append([\n",
    "                fit_time,\n",
    "                f1_score(y_test,y, average='macro'),\n",
    "                precision_score(y_test,y, average='macro'),\n",
    "                recall_score(y_test,y, average='macro'),\n",
    "                \n",
    "\n",
    "\n",
    "            ])\n",
    "    return res\n",
    "\n",
    "def print_table(res):\n",
    "       # Compute mean and std\n",
    "       final = {}     \n",
    "       for model in res:         \n",
    "         arr = np.array(res[model])         \n",
    "         final[model] = {             \n",
    "             \"time\" : arr[:, 0].mean().round(2),             \n",
    "             \"f1_score\": [arr[:,2].mean().round(3), arr[:,2].std().round(3)],             \n",
    "             \"Precision\" : arr[:,2].mean().round(3),             \n",
    "             \"Recall\" : arr[:,2].mean().round(3)         }\n",
    "       df = pd.DataFrame.from_dict(final, orient=\"index\").round(3)\n",
    "       return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = run_pipes([pipe1, pipe2, pipe3, pipe4, pipe5], splits=1)\n",
    "print_table(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transformer = LabelEncoder()\n",
    "#a = transformer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_test, y_train, y_test = train_test_split(targets, a, test_size=0.33, random_state=42)\n",
    "\n",
    "#clf = LinearSVC()\n",
    "#clf.fit(X_train, y_train)\n",
    "#y_pred = clf.predic(X_test)\n",
    "#print('F1 score :', f1_score(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
